# (PART) Otras consideraciones {-}

# Predictores Lineales

Sea $X_t$ un proceso estacionario. Se desea predecir $X_{n+1}$ en una función lineal de lo observado hasta el tiempo $n$ es decir con ${X_1,X_2,...,X_n}$.
Entonces lo que buscamos es encontrar los coeficientes $a_i$ con $i=1,...,n$ tales que

$$
\hat{X}_{n+1}=a_1X_n+a_2X_{n-1}+...+a_nX_1
$$

Utilizando las varianzas y covarianzas entre los elementos de la serie de tiempo se obtiene que una estimación de las $a_i's$ son las que den solución al siguiente sistema:

$$
\begin{bmatrix}
    Cov(X_1,X_1)       & Cov(X_1,X_2)  & Cov(X_1,X_3) & \dots & Cov(X_1,X_n)  \\
    Cov(X_2,X_1)        & Cov(X_2,X_2)  & Cov(X_2,X_3)  & \dots &Cov(X_2,X_n)  \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\ \\
    Cov(X_n,X_1)        & Cov(X_n,X_2)  & Cov(X_n,X_3)  & \dots & Cov(X_n,X_n) 
\end{bmatrix}
\begin{bmatrix}
a_1\\
a_2\\
\vdots\\
a_n\\
\end{bmatrix}
= \\
\begin{bmatrix}
Cov(X_{n+1},X_n)  \\
Cov(X_{n+1},X_{n-1})  \\
    \vdots\\
Cov(X_{n+1},X_1)  \\
\end{bmatrix}
$$

$$
\Gamma\underline{a} = \begin{bmatrix}
    \gamma_0      & \gamma_1  & \gamma_2 & \dots & \gamma_{n-1}  \\
   \gamma_1       & \gamma_0  & \gamma_1  & \dots &\gamma_{n-2}  \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\ \\
    \gamma_{n-1}       & \gamma_{n-2}  & \gamma_{n-3}  & \dots & \gamma_0 
\end{bmatrix}
\begin{bmatrix}
a_1\\
a_2\\
\vdots\\
a_n\\
\end{bmatrix}
=
\begin{bmatrix}
\gamma_1  \\
\gamma_2  \\
    \vdots\\
\gamma_{n}\\
\end{bmatrix} = \gamma
$$
En general si deseamos predecir $X_{n+h}$ dado ${X_1,X_2,...,X_n}$, el predictor lineal será

$$
\hat{X}_{n+h}=a_1X_n+a_2X_{n-1}+...+a_nX_1
$$

En donde el mejor predictor lineal serán los $a_1,a_2,...,a_n$ tales que 

$$
\begin{bmatrix}
    \gamma_0      & \gamma_1  & \gamma_2 & \dots & \gamma_{n-1}  \\
   \gamma_1       & \gamma_0  & \gamma_1  & \dots &\gamma_{n-2}  \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\ \\
    \gamma_{n-1}       & \gamma_{n-2}  & \gamma_{n-3}  & \dots & \gamma_0 
\end{bmatrix}
\begin{bmatrix}
a_1\\
a_2\\
\vdots\\
a_n\\
\end{bmatrix}
=
\begin{bmatrix}
\gamma_h  \\
\gamma_{h+1}  \\
    \vdots\\
\gamma_{h+n}  \\
\end{bmatrix}
$$
$$\Gamma \underline{a}=\gamma$$
**Ejemplo** ¿Cuál es el predictor lineal de $X_3$ dado $X_1, X_2$?

$$
\begin{bmatrix}
\gamma_0 & \gamma_1\\
\gamma_1 & \gamma_0
\end{bmatrix}
\begin{bmatrix}
a_1\\
a_2 
\end{bmatrix} = 
\begin{bmatrix}
\gamma_1\\
\gamma_2 
\end{bmatrix}
$$

Como $\rho_k = \frac{\gamma_k}{\gamma_0}$

$$
\begin{bmatrix}
1 & \rho_1\\
\rho_1 & 1
\end{bmatrix}
\begin{bmatrix}
a_1\\
a_2 
\end{bmatrix} = 
\begin{bmatrix}
\rho_1\\
\rho_2 
\end{bmatrix}
\implies
\begin{array}{l}
a_1+\rho_1a_2 = \rho_1\\
a1\rho_1+a_2 = \rho_2
\end{array}
$$
Y en general: $\hat{X}_{n+h} = a_1X_n+a_2X_{n-1}+\cdots+a_nX_1$.

**Ejercicios**

+ Sean $X_1$, $X_2$, $X_4$, $X_5$ obs. de un modelo $MA(2)$ de la forma $X_t = Z_t+0.5Z_{t-1}+0.25Z_{t-2}$. $Encontrar el mejor estimador para $X_3$ en términos de $X_1$ y $X_2$. $Z_t\sim N(0,1)$.
+ Sean $X_1$, $X_2$, $X_4$, $X_5$ obs. de un modelo $MA(2)$ de la forma $X_t = Z_t-0.25Z_{t-2}$. $Encontrar el mejor estimador para $X_3$ en términos de $X_2$ y $X_4$. $Z_t\sim N(0,1)$.


